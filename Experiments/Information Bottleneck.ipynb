{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19315,"status":"ok","timestamp":1711124072802,"user":{"displayName":"S N","userId":"07821467697827877075"},"user_tz":0},"id":"sV72npDrtyfs","outputId":"21ae4623-f6c1-4f15-c735-f6717199f776"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1a170f7ba77043c9b0c746a5df42d498","11ae3b56e63b426b9403414f457aa3e4","e4b61fdaea8b4dd4ae7466ab3f89b3a9","5457776188f346e787a8d8f3f4dfe801","9f4eed77b13c4bcd8073e568f2e8e65d","e3c684bb56ff43fcb0e738356cbe4f57","cc635d77197a43b5bb5b41554b767a1e","5c9436ebee2e42b6a6bcc270bb12e7c1","8684da481cad4ee0a400a3416a2c262c","ce9187411e5e4b978b3e983f6b54ea0a","24fea7973e544c2baed0aab990b309ff","397a5901a7af4486b9b15ed2cf27b687","c1d5ae217e554836839464531988d9b2","c6c8105ef73c47d69f14e935ee79dec6","297d1098b5a2449a8f215cf368a22d4b","4e515fa3809c42d2b1328c9c28a525ba","07a8e647e9c34b70aa1aee55e798e760","b9169d2de08140cd82621915b82dcda4","ab9868cb372245b683e24340974907c2","ea4aa86caa784418b0a93ebad3006086","f05ec8c87bd74b7181f94e1ac1282b78","cb42c3932d7f43898a66e3975b61acda","0588d10064d44818a92121fa5ba40031","87e1e6af36be4b798da0c280f84a4748","06580cd26f72476dbeb08ed801d10567","0bd860ba89f64d5982bea74a4bccbe49","f43a8f5ca3694a4c9feeddf3e6773c55","ff0549f071274661b6c91d7d21a66be2","7862bad0b3fd4cafa081bd9de767a90f","cab404a694b94a10a23e3fa0c6dec935","d902730c08fd445eb5fcaeb843cbec47","382cd37333fa4e4eb88b378093987a89","c8178d3788764e45950dd262c09c3191","af231846bb504ecb8e5b85e8a7f9ba1c","a7e2a97ba44440df8aec06c7fd29b865","4fd89b9096e444a6aabef4fc5283a116","d5c7d52ecf2e4f04af939a18e96b84c4","949f2329479e40749c4c569c83f3d2aa","f730d88100bd48b0af629a055773c1cd","93d0363fa3154f39a1e4ff2d1ea75831","0a1eacb0a6c54a3cb6bb78c245bb3689","4075af3153bc4f21bd71591284e7d56d","31e34ab242854f82b477ce8097e10848","5b9b813e61574384888efa9c8811f3e2","5ba4f26c0afa4c6d813cf0a3c5656ac6","c23681f1f5f445279f89c8a691c42e06","999833540ea94803b44e018f68e28c18","e0f674a2d9524f6d8e8ba0223d053326","1a9dbd3fb9564c888b46a8f02e3635df","dc7ff5d4e9a7407ba3ab0d19fef40639","79d82a9575fd40cc8b005d1842ed2535","c2c57a0e01fd4ee69adf4921e8e70abc","2efddb146b2440b2b00359cb7aa76039","586588eaf03541a7aa842ce97c9838a2","9277dd64a0da428a9c7c87870366b202","6acaf7770118428bae022f33d047e2ec","f8a97dac7d0344d89600a87110017993","dad6d4149b2d4474b4a77b0982b1cb4b","8b2bbb625ad8441dac7a23545193e3ca","46b55eed198148eb83d9770c5a4c64d4","b623b481e645499cb6ba0306e05b3e99","b044a26e3e7946ff823fb1d0ca28da4e","d645fba02ce847d3a2fab3deecc63461","0b8061ace6484a01b431064a2ca1e8aa","0bd6bc898fd54ccea903f8a143f927e3","5fe821a08aac400e968428ee69ca12f9","2538eff90a0c4bcb8ae37d7fe5bb240e","aa7c715b21fc495685505c46dec83fa2","0b29a29dccd14987981936e2795a491b","481e0d48a9fe4bf48029217d79078417","cce6f7de9aa44f7fa35369fd11273a31","5f79623a731e4db6a67b880a89940692","57022208047840f2b8606b3eccf378a0","1756446cb47b4367b4310b6292f00b2d","a57d301f85fb4dfb9bcc92ad52edb3dd","47befe9b40054c75a46faa603c84f933","46269bfbeb964beaafcefc92dfab1a79","686490febc03455e964d9c029e122ea6","8560bce07eea4fbf922c9d9bfd336836","0f5a583d67664eaaa9a3873784570d39","7da200dcb4864e22a0bc7305eaff2762","d80fce7d77914c56a277bfa721d64c3e","3f92d1eab4fe4051be42f94f95fa603a","a2c0c4ca790e43078f9525b5a2e2f0df","4f71b12315b944e6aac5b6ee687369e8","63bc1745bee6478da7889e7b4b62d319","3a64dab7ea184ac89aca20d87cadac55","60f04abd12714f2f8f0e36f582d3d17b","9badef381c4d46b3a808d69ebfb7bf00","5f407408b9d8483893039ddb69f50c46","ce3171da8d744341958a1df08ab2f7f0","7ceea24e42754cc0bf85e864dc8c23e9","10f19e7ecfa8477889e561cfa68901e5","ac0f73821cdc4495bd275c862c3b4f30","ba4f9b879728430887eaf0636b806e30","698cc30de8944d0e8f0032ef0310f243","57057461dcd34e8fa876e5365cb3488f","f65499bfb04f487fb39acd65322610bb","9d50ed9d9d6546d3a1af94adefc0648a","5be5c0e13db043ae9332ea4fc2dc94d8","9c46898a88ef45e190de307efb42abcc","c2e465a9d4444670aee46c7ea0e18682","be32a6a9715647c79d59864428446677","eb344e13f17f46f79b2928266fd5909f","5751c97ab3014905bc28043fc7e464df","1e95fe14d46b4a119f60babc1d117758","9daa286a2e3a4172901f8755d9553690","745aa4fdc14342729c8ff91483fe3af4","fa3e181ed9cb4d04b9add301eeded59b","e7916367e515408eba04879d12a48b8c","48bff7b04fd8460aa2d84c18327a8989","d5d955e72df04b4e86c9723ebcb5fb6c","1ab6f870e40344159cc478240855fb63","2eed00df550a4e12bcf3765628a765d5","ed862b5b711f4eb0ae70bc072d631174"]},"id":"q0rutX5PCkfL","outputId":"d244d54c-8685-49d7-ac1e-413adff70029"},"outputs":[],"source":["## Imports & Dataloading\n","\n","!pip install -q pytorch_lightning wandb einops\n","\n","import os\n","\n","import math\n","from math import sqrt\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","import torchvision\n","from torchvision import transforms\n","\n","import pytorch_lightning as L\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","import wandb\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","class CIFAR10DataModule(L.LightningDataModule):\n","    def __init__(self, batch_size):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizing in range [-1, 1] for all 3 channels\n","        ])\n","\n","    def prepare_data(self):\n","        if not os.path.exists('./data'):\n","            os.makedirs('./data')\n","\n","        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n","        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","    def setup(self, stage=None):\n","        if stage == 'fit' or stage is None:\n","            original_train = torchvision.datasets.CIFAR10(root='./data', train=True, transform=self.transform)\n","            self.cifar10_train, self.cifar10_val = torch.utils.data.random_split(original_train, [45000, 5000])\n","\n","        if stage == 'test' or stage is None:\n","            self.cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, transform=self.transform)\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_train, batch_size=self.batch_size, shuffle=True, num_workers=4)\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_val, batch_size=self.batch_size, num_workers=4)\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_test, batch_size=self.batch_size, num_workers=4)\n","\n","\n","## Global\n","\n","class TransformerBlock(nn.Module):\n","    ''' Transformer encoder block'''\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        '''\n","        Pre-norm formulation.\n","        Feed-forward hidden layer is 4x n_embed.\n","        '''\n","\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.self_attention = nn.MultiheadAttention(n_embed, num_heads, batch_first=True)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","\n","    def forward(self, x):\n","        norm_x = self.layer_norm_1(x)\n","        attention_out, attention_weights = self.self_attention(norm_x, norm_x, norm_x)\n","        x = x + attention_out\n","        x = x + self.mlp(self.layer_norm_2(x))\n","        return x\n","\n","\n","def _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n","    '''\n","    Generates a [1, length, channels] timing signal consisting of sinusoids\n","    Taken from:\n","    https://github.com/andreamad8/Universal-Transformer-Pytorch/blob/master/models/common_layer.py\n","    '''\n","\n","    position = np.arange(length)\n","    num_timescales = channels // 2\n","    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))\n","    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(float) * -log_timescale_increment)\n","    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n","\n","    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n","    signal = np.pad(signal, [[0, 0], [0, channels % 2]],\n","                    'constant', constant_values=[0.0, 0.0])\n","    signal =  signal.reshape([1, length, channels])\n","\n","    return torch.from_numpy(signal).type(torch.FloatTensor)\n","\n","## Encoder\n","\n","def select_token_pairs_for_merging(logits, r):\n","\n","    batch_size, num_tokens, _ = logits.size()\n","\n","    # We perform all operations on a cloned and detached version of the logits.\n","    # Gradients are obtained from the log-probs (outside of this function).\n","    masked_logits = logits.clone().detach().to(device='cuda')\n","    # We now mask the diagonal here since this was interferring with learning.\n","    masked_logits.diagonal(dim1=-2, dim2=-1).fill_(-float('inf'))\n","\n","    with torch.no_grad():\n","        # Initialize a mask to keep track of selected tokens\n","        mask = torch.zeros_like(logits, dtype=torch.bool, device='cuda')\n","\n","        # Tensor to store the pairs of tokens selected for merging\n","        selected_pairs = torch.zeros(batch_size, r, 2, dtype=torch.long, device='cuda')\n","\n","        # Tensor to track the indices selected in each batch\n","        indices_batch = torch.zeros(batch_size, r, dtype=torch.int64, device='cuda')\n","\n","        for pair_idx in range(r):\n","            # Set already selected tokens' similarities to -inf (becuase they then go into a softmax)\n","            masked_logits.masked_fill_(mask, float('-inf'))\n","\n","            # Apply Gumbel-Softmax (but we don't want any of their gradient approximations)\n","            gumbel_softmax_samples = torch.nn.functional.gumbel_softmax(masked_logits.view(batch_size, -1), tau=1, hard=True)\n","            indices = gumbel_softmax_samples.argmax(dim=1)\n","\n","            # Tracking the indices\n","            indices_batch[:, pair_idx] = indices\n","\n","            # No need to worry about gradients here, since \"indices\" was initialised in a torch.no_grad()\n","            rows = torch.div(indices, num_tokens, rounding_mode='trunc')\n","            cols = indices % num_tokens\n","\n","            # Store the selected token pairs\n","            selected_pairs[:, pair_idx, 0] = rows\n","            selected_pairs[:, pair_idx, 1] = cols\n","\n","            # Update the mask to avoid selecting these tokens again\n","            mask[torch.arange(batch_size), rows, :] = True\n","            mask[torch.arange(batch_size), :, cols] = True\n","            mask[torch.arange(batch_size), cols, :] = True\n","            mask[torch.arange(batch_size), :, rows] = True\n","\n","    return selected_pairs, indices_batch\n","\n","\n","class MergingBlock(nn.Module):\n","    ''' Merging block'''\n","\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        super().__init__()\n","\n","        # Where these q's (q_intial) are used for voting\n","        self.q_initial = nn.Linear(n_embed, n_embed, bias=None)\n","        self.kv = nn.Linear(n_embed, 2*n_embed, bias=None)\n","        # Where these q's (q_cross_attention) are used for cross attention post merging\n","        self.q = nn.Linear(n_embed, n_embed, bias=None)\n","\n","        # Learned token merging function\n","        self.merger = nn.Sequential(\n","            nn.Linear(2*n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Output projection\n","        self.out = nn.Linear(n_embed, n_embed)\n","\n","        # Layer norms\n","        self.layer_norm_q = nn.LayerNorm(n_embed)\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","\n","        # Transformer MLP\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Dropout\n","        self.attention_dropout = nn.Dropout(dropout)\n","        self.residual_dropout = nn.Dropout(dropout)\n","\n","        # Hyperparams\n","        self.n_embed = n_embed\n","        self.num_heads = num_heads\n","\n","        # For our weighted sum of the score matrices\n","        self.head_weights = nn.Parameter(torch.ones(num_heads))\n","\n","    def forward(self, x, r):\n","        # Batch, num_tokens, n_embed\n","        B, T, C = x.size()\n","\n","        assert r <= T // 2, 'r must be <= T/2'\n","\n","        q_initial = self.q_initial(self.layer_norm_q(x.detach()))\n","        k, v = self.kv(self.layer_norm_0(x)).split(self.n_embed, dim=2)\n","\n","        # -> (B, num_heads, num_tokens, head_dimension)\n","        q_initial = q_initial.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        k_detached = k.detach()\n","\n","        # Calculate attention scores\n","        scores = (q_initial @ k_detached.transpose(-2, -1)) * (1.0 / sqrt(k_detached.size(-1)))\n","\n","        # Combine the scores matricies with a learned weighted average\n","        normalised_head_weights = F.softmax(self.head_weights, dim=0)\n","        weighted_scores = scores * normalised_head_weights.view(1, self.num_heads, 1, 1)\n","        # Summing across the head dimension\n","        logits = weighted_scores.sum(dim=1)\n","\n","        # Obtain values for the l2 loss\n","        l2 = torch.sum(logits.view(B, -1), dim=1)\n","        l2 = l2**2\n","        l2 = torch.sum(l2, dim=0)\n","\n","        # This is the matrix from which we obtain the log probabilities\n","        log_policy_flat = F.log_softmax(logits.view(B, -1), dim=1)\n","\n","        # We now have logits and log_probs.\n","        # The logits will be used for sampling (via Gumbel-Softmax).\n","        # The log_probs will be backpropagated through using the indices obtained via sampling.\n","\n","        # Obtain pairs to merge\n","        # Here we pass our log_probs tensor through the function to update it with this set of actions\n","        pairs_batch, indices = select_token_pairs_for_merging(logits, r)\n","\n","        # Here we backprop through these decisions (later we manually re-scale the gradients by the full MSE loss)\n","        sampled_log_probs = torch.gather(log_policy_flat, 1, indices)\n","        merging_decision_loss = sampled_log_probs.sum()\n","\n","        # Helpful visualisation\n","        #make_dot(merging_decision_loss, params=dict(list(self.named_parameters()))).render(\"computation_graph\", format=\"png\")\n","\n","        # We don't want to try and backward when performing validation or inference\n","        if merging_decision_loss.requires_grad:\n","            merging_decision_loss.backward(retain_graph=True)\n","\n","        # Concatenate tokens to be merged\n","        merged_mask = torch.ones(B, T, dtype=torch.bool, device='cuda')\n","        pairs = x.gather(1, pairs_batch.view(B, -1).unsqueeze(-1).expand(-1, -1, self.n_embed))\n","        pairs = pairs.view(B, r, 2 * self.n_embed)\n","\n","        # Track which tokens are not being merged\n","        merged_mask.scatter_(1, pairs_batch.view(B, -1), False)\n","\n","        # Merge tokens\n","        merged_tokens = self.merger(pairs)\n","\n","        # Organise everything ready for output\n","        remaining_tokens = x[merged_mask].view(B, T - 2 * r, self.n_embed)\n","        # Our next layer of tokens (pre a final cross attention)\n","        x_prime = torch.cat([merged_tokens, remaining_tokens], dim=1)\n","\n","        # Performing cross-attention on our new representations\n","        q_cross_attention = self.q(self.layer_norm_1(x_prime))\n","        q_cross_attention = q_cross_attention.view(B, T-r, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        # Calculate attention weights\n","        att = (q_cross_attention @ k.transpose(-2, -1)) * (1.0 / sqrt(k.size(-1)))\n","        att = F.softmax(att, dim=-1)\n","        att = self.attention_dropout(att)\n","        y = att @ v # (B, num_heads, T, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n","        y = y.transpose(1, 2).contiguous().view(B, T-r, C) # re-assembe head outputs side by side\n","\n","        # Output projection for attention\n","        y = self.residual_dropout(self.out(y))\n","\n","        # Residual connections for attention and MLP outputs\n","        x_prime = x_prime + y\n","        x_prime = x_prime + self.mlp(self.layer_norm_2(x_prime))\n","\n","        return x_prime, l2\n","\n","# !pip install torchviz\n","# from torchviz import make_dot\n","\n","# tokens = torch.randn(2, 5, 6, device='cuda')\n","\n","# print(tokens)\n","\n","# print('---------------')\n","\n","# merging_block = MergingBlock(6, 3)\n","\n","# merging_block.to('cuda')\n","\n","# x, step_log_probs_1 = merging_block(tokens, 2)\n","\n","class PositionalImageEmbedding(nn.Module):\n","    def __init__(self, n_embed, image_size=(32,32), patch_size=(2,2), channels=3, bands=8):\n","        super().__init__()\n","        self.ff = self.fourier_features(image_size, bands)\n","\n","        image_height, image_width = image_size\n","        patch_height, patch_width = patch_size\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = (channels + 4*bands) * patch_height * patch_width\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, n_embed),\n","            nn.LayerNorm(n_embed),\n","        )\n","\n","        # Generate and register position encoding as a buffer\n","        position_encoding = self.fourier_features(image_size, bands)\n","        self.register_buffer(\"position_encoding\", position_encoding)\n","\n","    def fourier_features(self, shape, bands):\n","        height, width = shape\n","        y, x = torch.meshgrid(torch.linspace(-1, 1, height), torch.linspace(-1, 1, width))\n","\n","        # Linearly spaced frequencies\n","        # Minimum frequency for a full oscillation over the dimension\n","        min_freq = 1. / max(height, width)\n","        max_freq = min(height, width) / 2  # Nyquist frequency\n","        freqs = torch.linspace(min_freq, max_freq, steps=bands)\n","\n","        freqs_y = freqs.view(-1, 1, 1).repeat(1, height, 1)\n","        freqs_x = freqs.view(-1, 1, 1).repeat(1, width, 1)\n","\n","        embeddings = torch.cat([\n","            torch.sin(2 * math.pi * y * freqs_y),\n","            torch.cos(2 * math.pi * y * freqs_y),\n","            torch.sin(2 * math.pi * x * freqs_x),\n","            torch.cos(2 * math.pi * x * freqs_x)\n","        ], dim=0)\n","        return embeddings\n","\n","    def forward(self, img):\n","        # Initial x of shape [batch_size x channels x height x width]\n","        # Create position encoding of the same shape as x and move to the correct device\n","        batch_size = img.shape[0]\n","        enc = self.position_encoding.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","\n","        # Concatenate position encoding along the channel dimension\n","        # Shape is now [batch_size x (channels + 4*bands) x height x width]\n","        x = torch.cat([img, enc], dim=1)\n","\n","        # Reshape into a sequence of patches\n","        x = self.to_patch_embedding(x)\n","\n","        return x\n","\n","class EncoderArchitecture(nn.Module):\n","    def __init__(self, *, image_size, patch_size, fourier_bands, n_embed, num_layers, num_heads, channels=3, dropout=0.0):\n","        super().__init__()\n","\n","        assert n_embed % num_heads == 0, 'n_embed must be divisible by num_heads'\n","\n","        self.timing_signal = _gen_timing_signal(num_layers, n_embed).to('cuda')\n","\n","        self.to_patch_embedding = PositionalImageEmbedding(n_embed=n_embed, image_size=image_size, patch_size=patch_size, channels=channels, bands=fourier_bands)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.transformer_block = TransformerBlock(n_embed, num_heads, dropout)\n","        self.merging_block = MergingBlock(n_embed, num_heads, dropout)\n","\n","\n","    def forward(self, img, merging_schedule):\n","        x = self.to_patch_embedding(img)\n","\n","        B, T, C = x.shape\n","\n","        x = self.dropout(x)\n","\n","        l2_merging_policy = torch.zeros(1, requires_grad=True, device='cuda')\n","        l2_token_magnitude = 0\n","\n","        for l, r in enumerate(merging_schedule):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x = x + self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","            x = self.transformer_block(x)\n","            x, l2_policy = self.merging_block(x, r)\n","\n","            # Adding Guassian Noise drawn from a standard normal distribtution as part of our information bottleneck\n","            noise = torch.randn((B, x.shape[1], C)).to('cuda')\n","            x = x + noise\n","\n","            # Token magnitude loss\n","            token_norms = torch.norm(x, p=2, dim=2)\n","            l2_token_magnitude = l2_token_magnitude + token_norms.sum()\n","\n","            # Merging policy loss\n","            l2_merging_policy = l2_merging_policy + l2_policy\n","\n","        return x, l2_merging_policy, l2_token_magnitude\n","\n","\n","## Decoder\n","\n","class DecompositionBlock(nn.Module):\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        super().__init__()\n","\n","        self.scoring_function = nn.Sequential(\n","            nn.Linear(n_embed, n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(n_embed, 1),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.unmerger = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, 2*n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # --- Cross attention stuff ---\n","\n","        self.kv = nn.Linear(n_embed, 2*n_embed, bias=None)\n","        self.q = nn.Linear(n_embed, n_embed, bias=None)\n","        self.out = nn.Linear(n_embed, n_embed)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","\n","        self.attention_dropout = nn.Dropout(dropout)\n","        self.residual_dropout = nn.Dropout(dropout)\n","\n","        self.n_embed = n_embed\n","        self.num_heads = num_heads\n","\n","    def forward(self, x, r):\n","\n","        B, T, C = x.shape\n","\n","        # Keys & values for cross attention\n","        k, v = self.kv(self.layer_norm_0(x)).split(self.n_embed, dim=2)\n","        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, num_heads, num_tokens, head_dimension)\n","        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, num_heads, num_tokens, head_dimension)\n","\n","        # --- Unmerging process ---\n","\n","        # Scoring each token\n","        scores = self.scoring_function(x.detach())\n","\n","        # Computing L2 loss\n","        l2 = torch.sum(scores, dim=1)\n","        l2 = l2**2\n","        l2 = torch.sum(l2, dim=0)\n","\n","        policy = F.softmax(scores, dim=-2).squeeze(-1)\n","        log_policy = F.log_softmax(scores, dim=-2).squeeze(-1)\n","\n","        # Picking highest r to unmerge\n","        top_scores, indices = torch.topk(policy, r, dim=1)\n","\n","        # Accumulating gradients for the unmerging decision loss\n","        sampled_log_probs = torch.gather(log_policy, 1, indices)\n","        unmerging_decision_loss = sampled_log_probs.sum()\n","\n","        if unmerging_decision_loss.requires_grad:\n","            unmerging_decision_loss.backward(retain_graph=True)\n","\n","        # Mask to avoid duplication\n","        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n","        mask.scatter_(1, indices, False)\n","\n","        # Gathering tokens to unmerge\n","        indices = indices.unsqueeze(-1).expand(-1, -1, x.size(-1))\n","        top_tokens = torch.gather(x, 1, indices)\n","\n","        # Unmerging these tokens\n","        unmerged = self.unmerger(top_tokens)\n","\n","        # Reshape unmerged tokens: [batch_size, r, 2*n_embed] -> [B, 2*r, n_embed]\n","        unmerged = unmerged.view(B, r*2, self.n_embed)\n","\n","        # Mask out the original tokens that were unmerged\n","        remaining_tokens = x[mask].view(B, T - r, self.n_embed)\n","\n","        # Combine the original and unmerged tokens\n","        x_prime = torch.cat([remaining_tokens, unmerged], dim=1)\n","\n","        # --- Cross attention ---\n","\n","        q = self.q(self.layer_norm_1(x_prime))\n","        q = q.view(B, T+r, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        # Calculate attention weights\n","        att = (q @ k.transpose(-2, -1)) * (1.0 / sqrt(k.size(-1)))\n","        att = F.softmax(att, dim=-1)\n","        att = self.attention_dropout(att)\n","        y = att @ v # (B, num_heads, T+r, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T+r, head_size)\n","        y = y.transpose(1, 2).contiguous().view(B, T+r, C) # re-assembe head outputs side by side\n","\n","        # Output projection for attention\n","        y = self.residual_dropout(self.out(y))\n","\n","        # Residual connections for attention and MLP outputs\n","        x_prime = x_prime + y\n","        x_prime = x_prime + self.mlp(self.layer_norm_2(x_prime))\n","\n","        return x_prime, l2\n","\n","class DecoderArchitecture(nn.Module):\n","    def __init__(self, *, image_size, patch_size, fourier_bands, n_embed, num_layers, num_heads, channels=3, dropout=0.0):\n","        super().__init__()\n","\n","        self.timing_signal = _gen_timing_signal(num_layers, n_embed).to('cuda')\n","\n","        self.decomposition_block = DecompositionBlock(n_embed, num_heads, dropout=0.0)\n","        self.transformer_block = TransformerBlock(n_embed, num_heads, dropout)\n","        self.image_embedding = PositionalImageEmbedding(n_embed=n_embed, image_size=image_size, patch_size=patch_size, channels=channels, bands=fourier_bands)\n","        self.height_width = image_size[0]\n","\n","        # Cross attention to query pixel values from latent space\n","\n","        self.fourier_to_n_emb = nn.Linear(4*fourier_bands, n_embed)\n","\n","        self.self_attention = nn.MultiheadAttention(n_embed, num_heads, batch_first=True)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 2*n_embed),\n","            nn.GELU(),\n","            nn.Linear(2*n_embed, 3),\n","        )\n","\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","\n","        # To get the rgb values between -1 and 1\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x, merging_schedule):\n","        B, T, C = x.shape\n","\n","        l2_unmerging_policy = torch.zeros(1, requires_grad=True, device='cuda')\n","        l2_token_magnitude = 0\n","\n","        for l, r in enumerate(reversed(merging_schedule)):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x = x + self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","\n","            x, l2_policy = self.decomposition_block(x, r)\n","            x = self.transformer_block(x)\n","\n","            l2_unmerging_policy = l2_unmerging_policy + l2_policy\n","\n","            # Adding Guassian Noise drawn from a standard normal distribtution as part of our information bottleneck\n","            noise = torch.randn((B, x.shape[1], C)).to('cuda')\n","            x = x + noise\n","\n","            # Token magnitude loss\n","            token_norms = torch.norm(x, p=2, dim=2)\n","            l2_token_magnitude = l2_token_magnitude + token_norms.sum()\n","\n","        # Now we query this expanded latent with our handcrafted queries\n","        position_encoding = self.image_embedding.position_encoding\n","        position_encoding = rearrange(position_encoding, 'c h w -> (h w) c')\n","        position_encoding = self.fourier_to_n_emb(position_encoding) # [h*w, c] -> [h*w, n_embed]\n","        position_encoding = repeat(position_encoding, 'hw c -> b hw c', b=B)\n","\n","        norm_kv = self.layer_norm_0(x)\n","        norm_q = self.layer_norm_1(position_encoding)\n","\n","        # Compute MHA at n_embed\n","        attention_out, attention_weights = self.self_attention(norm_q, norm_kv, norm_kv)\n","\n","        # Reduce the dimentions to RGB predictions\n","        x = self.tanh(self.mlp(attention_out))\n","\n","        # Reshape to grid\n","        x = rearrange(x, 'b (h w) c -> b h w c', h=self.height_width, w=self.height_width)\n","\n","        return x, l2_unmerging_policy, l2_token_magnitude\n","\n","## Training\n","\n","class Model(L.LightningModule):\n","\n","    def __init__(self, model_kwargs, lr, complete_merging_schedule, rho_0, rho_step):\n","        super().__init__()\n","\n","        # Important: This property activates manual optimization.\n","        self.automatic_optimization = False\n","\n","        self.encoder = EncoderArchitecture(**model_kwargs)\n","        self.decoder = DecoderArchitecture(**model_kwargs)\n","\n","        self.rho = rho_0\n","        self.current_merging_schedule = complete_merging_schedule\n","\n","        self.save_hyperparameters('lr', 'complete_merging_schedule', 'rho_0', 'rho_step', 'model_kwargs')\n","\n","    def forward(self, x):\n","        compressed_representation, _, _ = self.encoder(x, self.current_merging_schedule)\n","        reconstruction, _, _ = self.decoder(compressed_representation, self.current_merging_schedule)\n","        return reconstruction\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n","        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n","        return [optimizer], [lr_scheduler]\n","\n","    def _calculate_loss(self, batch, mode='train'):\n","        images, _ = batch\n","\n","        compressed_representation, l2_encoder_policy, l2_encoder_token = self.encoder(images, self.current_merging_schedule)\n","        reconstructed, l2_decoder_policy, l2_decoder_token = self.decoder(compressed_representation, self.current_merging_schedule)\n","\n","        # Reshaping the images to match what the loss expects\n","        images = rearrange(images, 'b c h w -> b h w c')\n","\n","        reconstruction_loss = F.mse_loss(reconstructed, images, reduction='mean')\n","        l2_loss_policy = l2_encoder_policy + l2_decoder_policy\n","        l2_loss_token = l2_encoder_token + l2_decoder_token\n","\n","        self.log(f'{mode}_loss', reconstruction_loss)\n","        self.log(f'{mode}_l2_loss', l2_loss_policy)\n","        self.log(f'{mode}_l2_loss_token', l2_loss_token)\n","        self.log(f'schedule length', len(self.current_merging_schedule))\n","\n","        return reconstruction_loss, l2_loss_policy, l2_loss_token\n","\n","    def generate_reconstructions(self, batch, num_samples=10):\n","        images, _ = batch\n","        compressed_representation, _, _ = self.encoder(images, self.current_merging_schedule)\n","        reconstructed, _, _ = self.decoder(compressed_representation, self.current_merging_schedule)\n","\n","        # Select random samples\n","        indices = torch.randperm(images.size(0))[:num_samples]\n","        return images[indices], reconstructed[indices]\n","\n","    def log_reconstructions(self, batch, step_type='test'):\n","        original, reconstructed = self.generate_reconstructions(batch)\n","\n","        # torchvision.utils.make_grid expects channels first\n","        reconstructed = rearrange(reconstructed, 'b h w c -> b c h w')\n","\n","        # Convert tensors to grid of images\n","        original_grid = torchvision.utils.make_grid(original)\n","        reconstructed_grid = torchvision.utils.make_grid(reconstructed)\n","        # Log to wandb\n","        self.logger.experiment.log({\n","            f\"{step_type}_original_images\": wandb.Image(original_grid),\n","            f\"{step_type}_reconstructed_images\": wandb.Image(reconstructed_grid)\n","        })\n","\n","    def training_step(self, batch, batch_idx):\n","        optim = self.optimizers()\n","        reconstruction_loss, l2_loss_policy, l2_loss_token = self._calculate_loss(batch, mode='train')\n","\n","        # We manually scale the gradients of the q_intial and scoring_function networks by the reconstruction loss\n","        reconstruction_loss_scalar = reconstruction_loss.detach()\n","\n","        for param in self.encoder.merging_block.q_initial.parameters():\n","            param.grad = param.grad * reconstruction_loss_scalar\n","\n","        for param in self.decoder.decomposition_block.scoring_function.parameters():\n","            param.grad = param.grad * reconstruction_loss_scalar\n","\n","        wp = 1e-4\n","        wt = 1e-7\n","\n","        loss = reconstruction_loss + wp*l2_loss_policy + wt*l2_loss_token\n","\n","        # Backward after scaling\n","        self.manual_backward(loss)\n","\n","        # Step the optimiser\n","        optim.step()\n","\n","        # Zero grads after we step so that we can accumulate gradients in the forward pass\n","        optim.zero_grad()\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        # Validation set always uses the full merging schedule\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule\n","        reconstruction_loss, l2_loss, l2_loss_token = self._calculate_loss(batch, mode='val')\n","        loss = reconstruction_loss\n","\n","        if batch_idx == 0:  # Logging reconstruction for first batch\n","            self.log_reconstructions(batch, 'validation')\n","\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        # Test set always uses the full merging schedule\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule\n","        reconstruction_loss, l2_loss, l2_loss_token = self._calculate_loss(batch, mode='test')\n","        loss = reconstruction_loss\n","\n","    def adjust_merging_schedule(self):\n","        # Dynamically adjust the merging_schedule based on the current value of rho\n","        max_length = len(self.hparams.complete_merging_schedule)\n","        probabilities = [self.rho ** i for i in range(max_length)]\n","        schedule_length = 1 + sum(np.random.rand() < p for p in probabilities)\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule[:schedule_length]\n","\n","    def on_train_batch_start(self, batch, batch_idx):\n","        self.adjust_merging_schedule()\n","\n","    def on_train_epoch_end(self):\n","        self.rho = min(self.rho + self.hparams.rho_step, 1)  # Cap rho at 1\n","        self.log(f'rho', self.rho)\n","\n","\n","def train():\n","    # Hyperparameters\n","    BATCH_SIZE = 128\n","    EPOCHS = 100\n","    LR = 1e-4\n","    RHO_0 = 0.05                                            # Initial rho\n","    RHO_STEP = 0.01                                       # Per epoch\n","    COMPLETE_MERGING_SCHEDULE = [128, 64, 32, 16, 8, 4, 2]\n","    MODEL_KWARGS = {\n","        'image_size': (32,32),\n","        'patch_size': (2, 2),\n","        'fourier_bands': 8,\n","        'num_layers': 7,\n","        'n_embed': 256,\n","        'num_heads': 8,\n","        'dropout': 0.1,\n","    }\n","\n","    # Initialise data, model and logger\n","\n","    data = CIFAR10DataModule(batch_size=BATCH_SIZE)\n","    model = Model(MODEL_KWARGS, LR, COMPLETE_MERGING_SCHEDULE, RHO_0, RHO_STEP)\n","    checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')\n","    lr_monitor = LearningRateMonitor(logging_interval='step')\n","\n","    # Trainer setup\n","    trainer = Trainer(\n","        max_epochs=EPOCHS,\n","        logger=wandb_logger,\n","        callbacks=[checkpoint_callback, lr_monitor]\n","    )\n","\n","    # Start training\n","    trainer.fit(model, data)\n","\n","    torch.save(model.encoder.state_dict(), '')\n","    torch.save(model.decoder.state_dict(), '')\n","\n","    # Optionally: Test the model after training\n","    #trainer.test(model)\n","\n","wandb.login()\n","wandb_logger = WandbLogger(project='Reconstruction')\n","\n","train()\n","\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-xvEEjqo6Yv"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbn0ro1gtulf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNevmqpaBT1oXigUgEQ5D4/","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0588d10064d44818a92121fa5ba40031":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87e1e6af36be4b798da0c280f84a4748","IPY_MODEL_06580cd26f72476dbeb08ed801d10567","IPY_MODEL_0bd860ba89f64d5982bea74a4bccbe49"],"layout":"IPY_MODEL_f43a8f5ca3694a4c9feeddf3e6773c55"}},"06580cd26f72476dbeb08ed801d10567":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab404a694b94a10a23e3fa0c6dec935","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d902730c08fd445eb5fcaeb843cbec47","value":40}},"07a8e647e9c34b70aa1aee55e798e760":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a1eacb0a6c54a3cb6bb78c245bb3689":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b8061ace6484a01b431064a2ca1e8aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0bd6bc898fd54ccea903f8a143f927e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd860ba89f64d5982bea74a4bccbe49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_382cd37333fa4e4eb88b378093987a89","placeholder":"​","style":"IPY_MODEL_c8178d3788764e45950dd262c09c3191","value":" 40/40 [00:19&lt;00:00,  2.02it/s]"}},"11ae3b56e63b426b9403414f457aa3e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c684bb56ff43fcb0e738356cbe4f57","placeholder":"​","style":"IPY_MODEL_cc635d77197a43b5bb5b41554b767a1e","value":"Sanity Checking DataLoader 0: 100%"}},"1a170f7ba77043c9b0c746a5df42d498":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11ae3b56e63b426b9403414f457aa3e4","IPY_MODEL_e4b61fdaea8b4dd4ae7466ab3f89b3a9","IPY_MODEL_5457776188f346e787a8d8f3f4dfe801"],"layout":"IPY_MODEL_9f4eed77b13c4bcd8073e568f2e8e65d"}},"1a9dbd3fb9564c888b46a8f02e3635df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"1ab6f870e40344159cc478240855fb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e95fe14d46b4a119f60babc1d117758":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7916367e515408eba04879d12a48b8c","placeholder":"​","style":"IPY_MODEL_48bff7b04fd8460aa2d84c18327a8989","value":"Validation DataLoader 0: 100%"}},"24fea7973e544c2baed0aab990b309ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"297d1098b5a2449a8f215cf368a22d4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f05ec8c87bd74b7181f94e1ac1282b78","placeholder":"​","style":"IPY_MODEL_cb42c3932d7f43898a66e3975b61acda","value":" 20/352 [00:11&lt;03:02,  1.82it/s, v_num=wd4b]"}},"2eed00df550a4e12bcf3765628a765d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2efddb146b2440b2b00359cb7aa76039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31e34ab242854f82b477ce8097e10848":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"382cd37333fa4e4eb88b378093987a89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"397a5901a7af4486b9b15ed2cf27b687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1d5ae217e554836839464531988d9b2","IPY_MODEL_c6c8105ef73c47d69f14e935ee79dec6","IPY_MODEL_297d1098b5a2449a8f215cf368a22d4b"],"layout":"IPY_MODEL_4e515fa3809c42d2b1328c9c28a525ba"}},"4075af3153bc4f21bd71591284e7d56d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46b55eed198148eb83d9770c5a4c64d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"48bff7b04fd8460aa2d84c18327a8989":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e515fa3809c42d2b1328c9c28a525ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"4fd89b9096e444a6aabef4fc5283a116":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a1eacb0a6c54a3cb6bb78c245bb3689","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4075af3153bc4f21bd71591284e7d56d","value":40}},"5457776188f346e787a8d8f3f4dfe801":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce9187411e5e4b978b3e983f6b54ea0a","placeholder":"​","style":"IPY_MODEL_24fea7973e544c2baed0aab990b309ff","value":" 2/2 [00:04&lt;00:00,  0.47it/s]"}},"5751c97ab3014905bc28043fc7e464df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e95fe14d46b4a119f60babc1d117758","IPY_MODEL_9daa286a2e3a4172901f8755d9553690","IPY_MODEL_745aa4fdc14342729c8ff91483fe3af4"],"layout":"IPY_MODEL_fa3e181ed9cb4d04b9add301eeded59b"}},"586588eaf03541a7aa842ce97c9838a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b9b813e61574384888efa9c8811f3e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ba4f26c0afa4c6d813cf0a3c5656ac6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c23681f1f5f445279f89c8a691c42e06","IPY_MODEL_999833540ea94803b44e018f68e28c18","IPY_MODEL_e0f674a2d9524f6d8e8ba0223d053326"],"layout":"IPY_MODEL_1a9dbd3fb9564c888b46a8f02e3635df"}},"5c9436ebee2e42b6a6bcc270bb12e7c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fe821a08aac400e968428ee69ca12f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6acaf7770118428bae022f33d047e2ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8a97dac7d0344d89600a87110017993","IPY_MODEL_dad6d4149b2d4474b4a77b0982b1cb4b","IPY_MODEL_8b2bbb625ad8441dac7a23545193e3ca"],"layout":"IPY_MODEL_46b55eed198148eb83d9770c5a4c64d4"}},"745aa4fdc14342729c8ff91483fe3af4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eed00df550a4e12bcf3765628a765d5","placeholder":"​","style":"IPY_MODEL_ed862b5b711f4eb0ae70bc072d631174","value":" 40/40 [00:19&lt;00:00,  2.01it/s]"}},"7862bad0b3fd4cafa081bd9de767a90f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79d82a9575fd40cc8b005d1842ed2535":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8684da481cad4ee0a400a3416a2c262c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87e1e6af36be4b798da0c280f84a4748":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff0549f071274661b6c91d7d21a66be2","placeholder":"​","style":"IPY_MODEL_7862bad0b3fd4cafa081bd9de767a90f","value":"Validation DataLoader 0: 100%"}},"8b2bbb625ad8441dac7a23545193e3ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bd6bc898fd54ccea903f8a143f927e3","placeholder":"​","style":"IPY_MODEL_5fe821a08aac400e968428ee69ca12f9","value":" 40/40 [00:19&lt;00:00,  2.04it/s]"}},"9277dd64a0da428a9c7c87870366b202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93d0363fa3154f39a1e4ff2d1ea75831":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"949f2329479e40749c4c569c83f3d2aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"999833540ea94803b44e018f68e28c18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2c57a0e01fd4ee69adf4921e8e70abc","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2efddb146b2440b2b00359cb7aa76039","value":40}},"9daa286a2e3a4172901f8755d9553690":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d955e72df04b4e86c9723ebcb5fb6c","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ab6f870e40344159cc478240855fb63","value":40}},"9f4eed77b13c4bcd8073e568f2e8e65d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"a7e2a97ba44440df8aec06c7fd29b865":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f730d88100bd48b0af629a055773c1cd","placeholder":"​","style":"IPY_MODEL_93d0363fa3154f39a1e4ff2d1ea75831","value":"Validation DataLoader 0: 100%"}},"ab9868cb372245b683e24340974907c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af231846bb504ecb8e5b85e8a7f9ba1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7e2a97ba44440df8aec06c7fd29b865","IPY_MODEL_4fd89b9096e444a6aabef4fc5283a116","IPY_MODEL_d5c7d52ecf2e4f04af939a18e96b84c4"],"layout":"IPY_MODEL_949f2329479e40749c4c569c83f3d2aa"}},"b044a26e3e7946ff823fb1d0ca28da4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b623b481e645499cb6ba0306e05b3e99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9169d2de08140cd82621915b82dcda4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1d5ae217e554836839464531988d9b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07a8e647e9c34b70aa1aee55e798e760","placeholder":"​","style":"IPY_MODEL_b9169d2de08140cd82621915b82dcda4","value":"Epoch 43:   6%"}},"c23681f1f5f445279f89c8a691c42e06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc7ff5d4e9a7407ba3ab0d19fef40639","placeholder":"​","style":"IPY_MODEL_79d82a9575fd40cc8b005d1842ed2535","value":"Validation DataLoader 0: 100%"}},"c2c57a0e01fd4ee69adf4921e8e70abc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6c8105ef73c47d69f14e935ee79dec6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab9868cb372245b683e24340974907c2","max":352,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea4aa86caa784418b0a93ebad3006086","value":20}},"c8178d3788764e45950dd262c09c3191":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cab404a694b94a10a23e3fa0c6dec935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb42c3932d7f43898a66e3975b61acda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc635d77197a43b5bb5b41554b767a1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce9187411e5e4b978b3e983f6b54ea0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5c7d52ecf2e4f04af939a18e96b84c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31e34ab242854f82b477ce8097e10848","placeholder":"​","style":"IPY_MODEL_5b9b813e61574384888efa9c8811f3e2","value":" 40/40 [00:19&lt;00:00,  2.03it/s]"}},"d5d955e72df04b4e86c9723ebcb5fb6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d645fba02ce847d3a2fab3deecc63461":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d902730c08fd445eb5fcaeb843cbec47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dad6d4149b2d4474b4a77b0982b1cb4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d645fba02ce847d3a2fab3deecc63461","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b8061ace6484a01b431064a2ca1e8aa","value":40}},"dc7ff5d4e9a7407ba3ab0d19fef40639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0f674a2d9524f6d8e8ba0223d053326":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_586588eaf03541a7aa842ce97c9838a2","placeholder":"​","style":"IPY_MODEL_9277dd64a0da428a9c7c87870366b202","value":" 40/40 [00:19&lt;00:00,  2.01it/s]"}},"e3c684bb56ff43fcb0e738356cbe4f57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4b61fdaea8b4dd4ae7466ab3f89b3a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c9436ebee2e42b6a6bcc270bb12e7c1","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8684da481cad4ee0a400a3416a2c262c","value":2}},"e7916367e515408eba04879d12a48b8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea4aa86caa784418b0a93ebad3006086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed862b5b711f4eb0ae70bc072d631174":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f05ec8c87bd74b7181f94e1ac1282b78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f43a8f5ca3694a4c9feeddf3e6773c55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"f730d88100bd48b0af629a055773c1cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a97dac7d0344d89600a87110017993":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b623b481e645499cb6ba0306e05b3e99","placeholder":"​","style":"IPY_MODEL_b044a26e3e7946ff823fb1d0ca28da4e","value":"Validation DataLoader 0: 100%"}},"fa3e181ed9cb4d04b9add301eeded59b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ff0549f071274661b6c91d7d21a66be2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
