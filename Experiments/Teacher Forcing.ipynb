{"cells":[{"cell_type":"markdown","metadata":{"id":"n8h_LG99l3Nf"},"source":["## Imports & Dataloading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70136,"status":"ok","timestamp":1710938698955,"user":{"displayName":"S N","userId":"07821467697827877075"},"user_tz":0},"id":"ps5LaVdWGlKT","outputId":"79a8275b-a37b-464b-95a9-427b5feee248"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q pytorch_lightning wandb einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31Sa3dGwCpAD"},"outputs":[],"source":["import os\n","\n","import math\n","from math import sqrt\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","import torchvision\n","from torchvision import transforms\n","\n","import pytorch_lightning as L\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","import wandb\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFb5QTgFuEZm"},"outputs":[],"source":["class CIFAR10DataModule(L.LightningDataModule):\n","    def __init__(self, batch_size):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        # More transformations here in the future?\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizing in range [-1, 1] for all 3 channels\n","        ])\n","\n","    def prepare_data(self):\n","        if not os.path.exists('./data'):\n","            os.makedirs('./data')\n","\n","        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n","        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","    def setup(self, stage=None):\n","        if stage == 'fit' or stage is None:\n","            original_train = torchvision.datasets.CIFAR10(root='./data', train=True, transform=self.transform)\n","            self.cifar10_train, self.cifar10_val = torch.utils.data.random_split(original_train, [45000, 5000])\n","\n","        if stage == 'test' or stage is None:\n","            self.cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, transform=self.transform)\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_train, batch_size=self.batch_size, shuffle=True, num_workers=4)\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_val, batch_size=self.batch_size, num_workers=4)\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(self.cifar10_test, batch_size=self.batch_size, num_workers=4)\n"]},{"cell_type":"markdown","metadata":{"id":"h2aLmC5nZa6h"},"source":["## Global"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6MbLhvaI3Qg"},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    ''' Transformer encoder block'''\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        '''\n","        Pre-norm formulation.\n","        Feed-forward hidden layer is 4x n_embed.\n","        '''\n","\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.self_attention = nn.MultiheadAttention(n_embed, num_heads, batch_first=True)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","\n","    def forward(self, x):\n","        norm_x = self.layer_norm_1(x)\n","        attention_out, attention_weights = self.self_attention(norm_x, norm_x, norm_x)\n","        x = x + attention_out\n","        x = x + self.mlp(self.layer_norm_2(x))\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0r5NSq4iZeG8"},"outputs":[],"source":["def _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n","    '''\n","    Generates a [1, length, channels] timing signal consisting of sinusoids\n","    Taken from:\n","    https://github.com/andreamad8/Universal-Transformer-Pytorch/blob/master/models/common_layer.py\n","    '''\n","\n","    position = np.arange(length)\n","    num_timescales = channels // 2\n","    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))\n","    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(float) * -log_timescale_increment)\n","    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n","\n","    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n","    signal = np.pad(signal, [[0, 0], [0, channels % 2]],\n","                    'constant', constant_values=[0.0, 0.0])\n","    signal =  signal.reshape([1, length, channels])\n","\n","    return torch.from_numpy(signal).type(torch.FloatTensor)"]},{"cell_type":"markdown","metadata":{"id":"sIDMLE8Cl6pb"},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHZy7forgJLA"},"outputs":[],"source":["class PositionalImageEmbedding(nn.Module):\n","    def __init__(self, n_embed, image_size=(32,32), patch_size=(2,2), channels=3, bands=8):\n","        super().__init__()\n","        self.ff = self.fourier_features(image_size, bands)\n","\n","        image_height, image_width = image_size\n","        patch_height, patch_width = patch_size\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = (channels + 4*bands) * patch_height * patch_width\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, n_embed),\n","            nn.LayerNorm(n_embed),\n","        )\n","\n","        # Generate and register position encoding as a buffer\n","        position_encoding = self.fourier_features(image_size, bands)\n","        self.register_buffer(\"position_encoding\", position_encoding)\n","\n","    def fourier_features(self, shape, bands):\n","        height, width = shape\n","        y, x = torch.meshgrid(torch.linspace(-1, 1, height), torch.linspace(-1, 1, width))\n","\n","        # Linearly spaced frequencies\n","        # Minimum frequency for a full oscillation over the dimension\n","        min_freq = 1. / max(height, width)\n","        max_freq = min(height, width) / 2  # Nyquist frequency\n","        freqs = torch.linspace(min_freq, max_freq, steps=bands)\n","\n","        freqs_y = freqs.view(-1, 1, 1).repeat(1, height, 1)\n","        freqs_x = freqs.view(-1, 1, 1).repeat(1, width, 1)\n","\n","        embeddings = torch.cat([\n","            torch.sin(2 * math.pi * y * freqs_y),\n","            torch.cos(2 * math.pi * y * freqs_y),\n","            torch.sin(2 * math.pi * x * freqs_x),\n","            torch.cos(2 * math.pi * x * freqs_x)\n","        ], dim=0)\n","        return embeddings\n","\n","    def forward(self, img):\n","        # Initial x of shape [batch_size x channels x height x width]\n","        # Create position encoding of the same shape as x and move to the correct device\n","        batch_size = img.shape[0]\n","        enc = self.position_encoding.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","\n","        # Concatenate position encoding along the channel dimension\n","        # Shape is now [batch_size x (channels + 4*bands) x height x width]\n","        x = torch.cat([img, enc], dim=1)\n","\n","        # Reshape into a sequence of patches\n","        x = self.to_patch_embedding(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ntsyvxq3QrGD"},"outputs":[],"source":["def select_token_pairs_for_merging(logits, r):\n","    batch_size, num_tokens, _ = logits.size()\n","\n","    # We perform all operations on a cloned and detached version of the logits.\n","    # Gradients are obtained from the log-probs (outside of this function).\n","    masked_logits = logits.clone().detach().to(device='cuda')\n","    # Masking out the diagonal (score between a token and itself)\n","    masked_logits.diagonal(dim1=-2, dim2=-1).fill_(-float('inf'))\n","\n","    with torch.no_grad():\n","        # Initialize a mask to keep track of selected tokens\n","        mask = torch.zeros_like(logits, dtype=torch.bool, device='cuda')\n","\n","        # Tensor to store the pairs of tokens selected for merging\n","        selected_pairs = torch.zeros(batch_size, r, 2, dtype=torch.long, device='cuda')\n","\n","        # Tensor to track the indices selected in each batch\n","        indices_batch = torch.zeros(batch_size, r, dtype=torch.int64, device='cuda')\n","\n","        for pair_idx in range(r):\n","            # Set already selected tokens' similarities to -inf (becuase they then go into a softmax)\n","            masked_logits.masked_fill_(mask, float('-inf'))\n","\n","            # Apply Gumbel-Softmax (but we don't want any of their gradient approximations)\n","            gumbel_softmax_samples = torch.nn.functional.gumbel_softmax(masked_logits.view(batch_size, -1), tau=1, hard=True)\n","            indices = gumbel_softmax_samples.argmax(dim=1)\n","\n","            # Tracking the indices\n","            indices_batch[:, pair_idx] = indices\n","\n","            # No need to worry about gradients here, since \"indices\" was initialised in a torch.no_grad()\n","            rows = torch.div(indices, num_tokens, rounding_mode='trunc')\n","            cols = indices % num_tokens\n","\n","            # Store the selected token pairs\n","            selected_pairs[:, pair_idx, 0] = rows\n","            selected_pairs[:, pair_idx, 1] = cols\n","\n","            # Update the mask to avoid selecting these tokens again\n","            mask[torch.arange(batch_size), rows, :] = True\n","            mask[torch.arange(batch_size), :, cols] = True\n","            mask[torch.arange(batch_size), cols, :] = True\n","            mask[torch.arange(batch_size), :, rows] = True\n","\n","    return selected_pairs, indices_batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHUiHZ9CoT4f"},"outputs":[],"source":["class MergingBlock(nn.Module):\n","    ''' Merging block'''\n","\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        super().__init__()\n","\n","        # Where these q's (q_intial) are used for voting\n","        self.q_initial = nn.Linear(n_embed, n_embed, bias=None)\n","        self.kv = nn.Linear(n_embed, 2*n_embed, bias=None)\n","        # Where these q's (q_cross_attention) are used for cross attention post merging\n","        self.q = nn.Linear(n_embed, n_embed, bias=None)\n","\n","        # Learned token merging function\n","        self.merger = nn.Sequential(\n","            nn.Linear(2*n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Output projection\n","        self.out = nn.Linear(n_embed, n_embed)\n","\n","        # Layer norms\n","        self.layer_norm_q = nn.LayerNorm(n_embed)\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","\n","        # Transformer MLP\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Dropout\n","        self.attention_dropout = nn.Dropout(dropout)\n","        self.residual_dropout = nn.Dropout(dropout)\n","\n","        # Hyperparams\n","        self.n_embed = n_embed\n","        self.num_heads = num_heads\n","\n","        # For our weighted sum of the score matrices\n","        self.head_weights = nn.Parameter(torch.ones(num_heads))\n","\n","    def forward(self, x, x_original, r):\n","        # Batch, num_tokens, n_embed\n","        B, T, C = x.size()\n","\n","        assert r <= T // 2, 'r must be <= T/2'\n","\n","        q_initial = self.q_initial(self.layer_norm_q(x.detach()))\n","        k, v = self.kv(self.layer_norm_0(x)).split(self.n_embed, dim=2)\n","\n","        # -> (B, num_heads, num_tokens, head_dimension)\n","        q_initial = q_initial.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        k_detached = k.detach()\n","\n","        # Calculate attention scores\n","        scores = (q_initial @ k_detached.transpose(-2, -1)) * (1.0 / sqrt(k_detached.size(-1)))\n","\n","        # Combine the scores matricies with a learned weighted average\n","        normalised_head_weights = F.softmax(self.head_weights, dim=0)\n","        weighted_scores = scores * normalised_head_weights.view(1, self.num_heads, 1, 1)\n","        # Summing across the head dimension\n","        logits = weighted_scores.sum(dim=1)\n","\n","        # This is the matrix from which we obtain the log probabilities\n","        log_policy_flat = F.log_softmax(logits.view(B, -1), dim=1)\n","\n","        # We now have logits and log_probs.\n","        # The logits will be used for sampling (via Gumbel-Softmax).\n","        # The log_probs will be backpropagated through using the indices obtained via sampling.\n","\n","        # Obtain pairs to merge\n","        # Here we pass our log_probs tensor through the function to update it with this set of actions\n","        pairs_batch, indices = select_token_pairs_for_merging(logits, r)\n","\n","        # Here we backprop through these decisions (later we manually re-scale the gradients by the full MSE loss)\n","        sampled_log_probs = torch.gather(log_policy_flat, 1, indices)\n","        merging_decision_loss = sampled_log_probs.sum()\n","\n","        # Concatenate tokens to be merged\n","        merged_mask = torch.ones(B, T, dtype=torch.bool, device='cuda')\n","        pairs = x.gather(1, pairs_batch.view(B, -1).unsqueeze(-1).expand(-1, -1, self.n_embed))\n","        pairs = pairs.view(B, r, 2 * self.n_embed)\n","\n","        pairs_original = x_original.gather(1, pairs_batch.view(B, -1).unsqueeze(-1).expand(-1, -1, self.n_embed))\n","        pairs_original = pairs_original.view(B, r, 2 * self.n_embed)\n","\n","        # Track which tokens are not being merged\n","        merged_mask.scatter_(1, pairs_batch.view(B, -1), False)\n","\n","        # Merge tokens\n","        merged_tokens = self.merger(pairs)\n","\n","        # Organise everything ready for output\n","        remaining_tokens = x[merged_mask].view(B, T - 2 * r, self.n_embed)\n","        remaining_original = x_original[merged_mask].view(B, T - 2 * r, self.n_embed)\n","\n","        # Indicies of the tokens that were not merged\n","        _, token_indices = torch.where(merged_mask)\n","        unmerged_tracker = token_indices.view(B, T - 2 * r, 1)\n","\n","        # Our next layer of tokens (pre a final cross attention)\n","        x_prime = torch.cat([merged_tokens, remaining_tokens], dim=1)\n","        # Our original x values, but rearranged such that we can easily calculate token-wise reconstruction loss\n","        x_targets = torch.cat([pairs_original.view(B, 2*r, self.n_embed), remaining_original], dim=1)\n","\n","        # --- Cross attention ---\n","\n","        # Performing cross-attention on our new representations\n","        q_cross_attention = self.q(self.layer_norm_1(x_prime))\n","        q_cross_attention = q_cross_attention.view(B, T-r, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        # Calculate attention weights\n","        att = (q_cross_attention @ k.transpose(-2, -1)) * (1.0 / sqrt(k.size(-1)))\n","        att = F.softmax(att, dim=-1)\n","        att = self.attention_dropout(att)\n","        y = att @ v # (B, num_heads, T, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n","        y = y.transpose(1, 2).contiguous().view(B, T-r, C) # re-assembe head outputs side by side\n","\n","        # Output projection for attention\n","        y = self.residual_dropout(self.out(y))\n","\n","        # Residual connections for attention and MLP outputs\n","        x_prime = x_prime + y\n","        x_prime = x_prime + self.mlp(self.layer_norm_2(x_prime))\n","\n","        return x_prime, x_targets, merging_decision_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIdXHmjRpsYk"},"outputs":[],"source":["class DecompositionBlock(nn.Module):\n","    def __init__(self, n_embed, num_heads, dropout=0.0):\n","        super().__init__()\n","\n","        self.scoring_function = nn.Sequential(\n","            nn.Linear(n_embed, n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(n_embed, 1),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.unmerger = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, 2*n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # --- Cross attention stuff ---\n","\n","        self.kv = nn.Linear(n_embed, 2*n_embed, bias=None)\n","        self.q = nn.Linear(n_embed, n_embed, bias=None)\n","        self.out = nn.Linear(n_embed, n_embed)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(4*n_embed, n_embed),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","\n","        self.attention_dropout = nn.Dropout(dropout)\n","        self.residual_dropout = nn.Dropout(dropout)\n","\n","        self.n_embed = n_embed\n","        self.num_heads = num_heads\n","\n","    def forward(self, x, r, inference=False):\n","\n","        B, T, C = x.shape\n","\n","        # Keys & values for cross attention\n","        k, v = self.kv(self.layer_norm_0(x)).split(self.n_embed, dim=2)\n","        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, num_heads, num_tokens, head_dimension)\n","        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, num_heads, num_tokens, head_dimension)\n","\n","        # --- Unmerging process ---\n","\n","        # Scoring each token\n","        scores = self.scoring_function(x.detach())\n","\n","        policy = F.softmax(scores, dim=-2).squeeze(-1)\n","        log_policy = F.log_softmax(scores, dim=-2).squeeze(-1)\n","\n","        # Picking highest r to unmerge\n","        if inference:\n","            indices = torch.multinomial(policy, r, replacement=False)\n","            indices, _ = torch.sort(indices, dim=1)\n","        else:\n","            # Teacher-forcing\n","            indices = torch.arange(r).expand(B, r).to('cuda')\n","\n","        # Accumulating gradients for the unmerging decision loss\n","        sampled_log_probs = torch.gather(log_policy, 1, indices)\n","        unmerging_decision_log_likelihood = sampled_log_probs.sum()\n","        unmerging_decision_loss = -unmerging_decision_log_likelihood\n","\n","        # Mask to avoid duplication\n","        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n","        mask.scatter_(1, indices, False)\n","\n","        # Gathering tokens to unmerge\n","        indices = indices.unsqueeze(-1).expand(-1, -1, x.size(-1))\n","        top_tokens = torch.gather(x, 1, indices)\n","\n","        # Unmerging these tokens\n","        unmerged = self.unmerger(top_tokens)\n","\n","        # Reshape unmerged tokens: [batch_size, r, 2*n_embed] -> [B, 2*r, n_embed]\n","        unmerged = unmerged.view(B, r*2, self.n_embed)\n","\n","        # Mask out the original tokens that were unmerged\n","        remaining_tokens = x[mask].view(B, T - r, self.n_embed)\n","\n","        # Combine the original and unmerged tokens\n","        x_prime = torch.cat([unmerged, remaining_tokens], dim=1)\n","\n","        # --- Cross attention ---\n","\n","        q = self.q(self.layer_norm_1(x_prime))\n","        q = q.view(B, T+r, self.num_heads, C // self.num_heads).transpose(1, 2)\n","\n","        # Calculate attention weights\n","        att = (q @ k.transpose(-2, -1)) * (1.0 / sqrt(k.size(-1)))\n","        att = F.softmax(att, dim=-1)\n","        att = self.attention_dropout(att)\n","        y = att @ v # (B, num_heads, T+r, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T+r, head_size)\n","        y = y.transpose(1, 2).contiguous().view(B, T+r, C) # re-assembe head outputs side by side\n","\n","        # Output projection for attention\n","        y = self.residual_dropout(self.out(y))\n","\n","        # Residual connections for attention and MLP outputs\n","        x_prime = x_prime + y\n","        x_prime = x_prime + self.mlp(self.layer_norm_2(x_prime))\n","\n","        return x_prime, unmerging_decision_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7037,"status":"ok","timestamp":1710938713569,"user":{"displayName":"S N","userId":"07821467697827877075"},"user_tz":0},"id":"IPyKIVBuDvHV","outputId":"cb00610a-39a9-4475-94cd-6c1fae4093cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.2.1+cu121)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz) (12.4.99)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=0eeea552ff2601cb821c9632a0cbec7f0d92a33ee9d432f4929fd45cbd67aa17\n","  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}],"source":["!pip install torchviz\n","from torchviz import make_dot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhrK1l8EUXhn"},"outputs":[],"source":["class Architecture(nn.Module):\n","    def __init__(self, *, image_size, patch_size, fourier_bands, n_embed, num_layers, num_heads, channels=3, dropout=0.0):\n","        super().__init__()\n","\n","        assert n_embed % num_heads == 0, 'n_embed must be divisible by num_heads'\n","\n","        self.num_layers = num_layers\n","\n","        self.timing_signal = _gen_timing_signal(self.num_layers*2, n_embed).to('cuda')\n","\n","        # Encoder\n","\n","        self.to_patch_embedding = PositionalImageEmbedding(n_embed=n_embed, image_size=image_size, patch_size=patch_size, channels=channels, bands=fourier_bands)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.transformer_block_encoder = TransformerBlock(n_embed, num_heads, dropout)\n","        self.merging_block = MergingBlock(n_embed, num_heads, dropout)\n","\n","        # Decoder\n","\n","        self.decomposition_block = DecompositionBlock(n_embed, num_heads, dropout=0.0)\n","        self.transformer_block_decoder = TransformerBlock(n_embed, num_heads, dropout)\n","        self.image_embedding = PositionalImageEmbedding(n_embed=n_embed, image_size=image_size, patch_size=patch_size, channels=channels, bands=fourier_bands)\n","        self.height_width = image_size[0]\n","\n","        # Cross attention to query pixel values from latent space\n","\n","        self.fourier_to_n_emb = nn.Linear(4*fourier_bands, n_embed)\n","\n","        self.self_attention = nn.MultiheadAttention(n_embed, num_heads, batch_first=True)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(n_embed, 2*n_embed),\n","            nn.GELU(),\n","            nn.Linear(2*n_embed, 3),\n","        )\n","\n","        self.layer_norm_0 = nn.LayerNorm(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","\n","        # To get the rgb values between -1 and 1\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x, merging_schedule):\n","\n","        x = self.to_patch_embedding(x)\n","\n","        B, T, C = x.shape\n","\n","        x = self.dropout(x)\n","\n","        x_0 = x.clone().detach()\n","\n","        for l, r in enumerate(merging_schedule):\n","\n","            x = x.detach()\n","\n","            # Making sure that everything needs gradients again\n","            self.transformer_block_encoder.requires_grad = True\n","            self.transformer_block_decoder.requires_grad = True\n","            self.merging_block.requires_grad = True\n","            self.decomposition_block.requires_grad = True\n","\n","            # Encoder step\n","            x += self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","            x_original = x.clone().detach()\n","            x = self.transformer_block_encoder(x)\n","            x, x_targets, merging_decision_loss = self.merging_block(x, x_original, r)\n","\n","            # Decoder step\n","            x += self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","            x, unmerging_decision_loss = self.decomposition_block(x, r)\n","            x = self.transformer_block_decoder(x)\n","\n","            # Loss term 1 - reconstruction loss\n","            reconstruction_loss = F.mse_loss(x, x_targets, reduction='mean')\n","            reconstruction_loss_scalar = reconstruction_loss.detach()\n","\n","            if reconstruction_loss.requires_grad:\n","                #make_dot(reconstruction_loss, params=dict(list(self.named_parameters()))).render(\"computation_graph_recon\", format=\"png\")\n","                reconstruction_loss.backward()\n","\n","            # Loss term 2 - unmerging decision\n","            unmerging_decision_loss_scalar = unmerging_decision_loss.detach()\n","            if unmerging_decision_loss.requires_grad:\n","                #make_dot(unmerging_decision_loss, params=dict(list(self.named_parameters()))).render(\"computation_graph_unmer\", format=\"png\")\n","                unmerging_decision_loss.backward()\n","\n","            # Loss term 3 - merging decision\n","            if merging_decision_loss.requires_grad:\n","                #make_dot(merging_decision_loss, params=dict(list(self.named_parameters()))).render(\"computation_graph_mer\", format=\"png\")\n","                (merging_decision_loss * (reconstruction_loss_scalar + unmerging_decision_loss_scalar)).backward()\n","\n","        x = x_0\n","\n","        # Here we then perform a full forward & backward pass\n","\n","        for l, r in enumerate(merging_schedule):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x += self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","\n","            x = self.transformer_block_encoder(x)\n","            x, _, _ = self.merging_block(x, x, r)\n","\n","        # Decode\n","\n","        for l, r in enumerate(reversed(merging_schedule)):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x += self.timing_signal[:, self.num_layers-l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","\n","            x, _ = self.decomposition_block(x, r)\n","            x = self.transformer_block_decoder(x)\n","\n","        # Now we query this expanded latent with our handcrafted queries\n","        position_encoding = self.image_embedding.position_encoding\n","        position_encoding = rearrange(position_encoding, 'c h w -> (h w) c')\n","        position_encoding = self.fourier_to_n_emb(position_encoding) # [h*w, c] -> [h*w, n_embed]\n","        position_encoding = repeat(position_encoding, 'hw c -> b hw c', b=B)\n","\n","        norm_kv = self.layer_norm_0(x)\n","        norm_q = self.layer_norm_1(position_encoding)\n","\n","        # Compute MHA at n_embed\n","        attention_out, attention_weights = self.self_attention(norm_q, norm_kv, norm_kv)\n","\n","        # Reduce the dimentions to RGB predictions\n","        x = self.tanh(self.mlp(attention_out))\n","\n","        # Reshape to grid\n","        x = rearrange(x, 'b (h w) c -> b h w c', h=self.height_width, w=self.height_width)\n","\n","        return x\n","\n","    def inference(self, x, merging_schedule):\n","\n","        # Encode\n","\n","        x = self.to_patch_embedding(x)\n","\n","        B, T, C = x.shape\n","\n","        x = self.dropout(x)\n","\n","        for l, r in enumerate(merging_schedule):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x += self.timing_signal[:, l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","\n","            x = self.transformer_block_encoder(x)\n","            x, _, _ = self.merging_block(x, x, r)\n","\n","        # Decode\n","\n","        for l, r in enumerate(reversed(merging_schedule)):\n","            # Signal to allow the network to differentiate between layers (broadcasts along batch dimension)\n","            x += self.timing_signal[:, self.num_layers-l, :].unsqueeze(1).repeat(1, x.shape[1], 1)\n","\n","            x, _ = self.decomposition_block(x, r)\n","            x = self.transformer_block_decoder(x)\n","\n","        # Now we query this expanded latent with our handcrafted queries\n","        position_encoding = self.image_embedding.position_encoding\n","        position_encoding = rearrange(position_encoding, 'c h w -> (h w) c')\n","        position_encoding = self.fourier_to_n_emb(position_encoding) # [h*w, c] -> [h*w, n_embed]\n","        position_encoding = repeat(position_encoding, 'hw c -> b hw c', b=B)\n","\n","        norm_kv = self.layer_norm_0(x)\n","        norm_q = self.layer_norm_1(position_encoding)\n","\n","        # Compute MHA at n_embed\n","        attention_out, attention_weights = self.self_attention(norm_q, norm_kv, norm_kv)\n","\n","        # Reduce the dimentions to RGB predictions\n","        x = self.tanh(self.mlp(attention_out))\n","\n","        # Reshape to grid\n","        x = rearrange(x, 'b (h w) c -> b h w c', h=self.height_width, w=self.height_width)\n","\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5zP9n60ImSf-"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ntAEeMrgLHR"},"outputs":[],"source":["class Model(L.LightningModule):\n","\n","    def __init__(self, model_kwargs, lr, complete_merging_schedule, rho_0, rho_step):\n","        super().__init__()\n","\n","        # Important: This property activates manual optimization.\n","        self.automatic_optimization = False\n","\n","        self.model = Architecture(**model_kwargs)\n","\n","        self.rho = rho_0\n","        self.current_merging_schedule = complete_merging_schedule\n","\n","        self.save_hyperparameters('lr', 'complete_merging_schedule', 'rho_0', 'rho_step', 'model_kwargs')\n","\n","    def forward(self, x):\n","        return self.model(x, self.current_merging_schedule)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n","        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n","        return [optimizer], [lr_scheduler]\n","\n","    def _calculate_loss(self, batch, mode='train'):\n","        images, _ = batch\n","\n","        if mode == 'val':\n","            reconstructed = self.model.inference(images, self.current_merging_schedule)\n","\n","            # Reshaping the images to match what the loss expects\n","            images = rearrange(images, 'b c h w -> b h w c')\n","            loss = F.mse_loss(reconstructed, images, reduction='mean')\n","\n","            self.log(f'{mode}_loss', loss)\n","            self.log(f'schedule length', len(self.current_merging_schedule))\n","            return loss\n","\n","        reconstructed = self.model(images, self.current_merging_schedule)\n","\n","        # Reshaping the images to match what the loss expects\n","        images = rearrange(images, 'b c h w -> b h w c')\n","        loss = F.mse_loss(reconstructed, images, reduction='mean')\n","\n","        self.log(f'{mode}_loss', loss)\n","        self.log(f'schedule length', len(self.current_merging_schedule))\n","        return loss\n","\n","    def generate_reconstructions(self, batch, num_samples=10):\n","        images, _ = batch\n","        reconstructed = self.model.inference(images, self.current_merging_schedule)\n","\n","        # Select random samples\n","        indices = torch.randperm(images.size(0))[:num_samples]\n","        return images[indices], reconstructed[indices]\n","\n","    def log_reconstructions(self, batch, step_type='test'):\n","        original, reconstructed = self.generate_reconstructions(batch)\n","\n","        # torchvision.utils.make_grid expects channels first\n","        reconstructed = rearrange(reconstructed, 'b h w c -> b c h w')\n","\n","        # Convert tensors to grid of images\n","        original_grid = torchvision.utils.make_grid(original)\n","        reconstructed_grid = torchvision.utils.make_grid(reconstructed)\n","        # Log to wandb\n","        self.logger.experiment.log({\n","            f\"{step_type}_original_images\": wandb.Image(original_grid),\n","            f\"{step_type}_reconstructed_images\": wandb.Image(reconstructed_grid)\n","        })\n","\n","    def training_step(self, batch, batch_idx):\n","        optim = self.optimizers()\n","\n","        # This computes the loss for the fourier layer\n","        loss = self._calculate_loss(batch, mode='train')\n","        self.manual_backward(loss)\n","\n","        # Step the optimiser\n","        optim.step()\n","\n","        # Zero grads after we step so that we can accumulate gradients in the forward pass\n","        optim.zero_grad()\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        # Validation set always uses the full merging schedule\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule\n","        loss = self._calculate_loss(batch, mode='val')\n","\n","        if batch_idx == 0:  # Logging reconstruction for first batch\n","            self.log_reconstructions(batch, 'validation')\n","\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        # Test set always uses the full merging schedule\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule\n","        loss = self._calculate_loss(batch, mode='test')\n","\n","    def adjust_merging_schedule(self):\n","        # Dynamically adjust the merging_schedule based on the current value of rho\n","        max_length = len(self.hparams.complete_merging_schedule)\n","        probabilities = [self.rho ** i for i in range(max_length)]\n","        schedule_length = 1 + sum(np.random.rand() < p for p in probabilities)\n","        self.current_merging_schedule = self.hparams.complete_merging_schedule[:schedule_length]\n","\n","    def on_train_batch_start(self, batch, batch_idx):\n","        self.adjust_merging_schedule()\n","\n","    def on_train_epoch_end(self):\n","        self.rho = min(self.rho + self.hparams.rho_step, 1)  # Cap rho at 1\n","        self.log(f'rho', self.rho)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A82KF2r3ssZo"},"outputs":[],"source":["def train():\n","    # Hyperparameters\n","    BATCH_SIZE = 128\n","    EPOCHS = 200\n","    LR = 1e-4\n","    RHO_0 = 1.                                            # Initial rho\n","    RHO_STEP = 0.01                                       # Per epoch\n","    COMPLETE_MERGING_SCHEDULE = [128, 64, 32, 16, 8, 4, 2]\n","    MODEL_KWARGS = {\n","        'image_size': (32,32),\n","        'patch_size': (2, 2),\n","        'fourier_bands': 8,\n","        'num_layers': 7,\n","        'n_embed': 256,\n","        'num_heads': 8,\n","        'dropout': 0.1,\n","    }\n","\n","    # Initialise data, model and logger\n","\n","    data = CIFAR10DataModule(batch_size=BATCH_SIZE)\n","    model = Model(MODEL_KWARGS, LR, COMPLETE_MERGING_SCHEDULE, RHO_0, RHO_STEP)\n","    checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')\n","    lr_monitor = LearningRateMonitor(logging_interval='step')\n","\n","    # Trainer setup\n","    trainer = Trainer(\n","        max_epochs=EPOCHS,\n","        logger=wandb_logger,\n","        callbacks=[checkpoint_callback, lr_monitor]\n","    )\n","\n","    # Start training\n","    trainer.fit(model, data)\n","\n","    torch.save(model.model.state_dict(), 'model_weights_teacher_forcing')\n","\n","    # Optionally: Test the model after training\n","    #trainer.test(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":7083,"status":"ok","timestamp":1710938720647,"user":{"displayName":"S N","userId":"07821467697827877075"},"user_tz":0},"id":"mCVkxafkpWKd","outputId":"a71c8851-c439-4a4c-de71-4a6692bde1de"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["wandb.login()\n","wandb_logger = WandbLogger(project='Reconstruction')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":538,"referenced_widgets":["22ce04bd9c424885b5170eb807e88000","ac10c65c1f9a4637a80bbb37fa97bd99","c0de1aec2aa342aeb1ad23f71858412f","218563f8f8a849b69b92053370fcce79","32f47b5e376c44edb9657f669322c0f1","a33cbd7efa464c6e8d76c27d3a0d3fec","3662572bc60a4d4f9219f3680c0f15b1","26bc5cf831b54ecab4bdfda72b14d785","b26a59549f5f420598cd5c3dc7bf14c4","f7df737d98f140159597180c82ae9b6b","e9a2229058cd4caabf47b6644a05eafc","a18eb1f168d443e680ccea951fde3529","f592a0b632384704a673df9a7ba87c2a","03a5b874b7874023b6021b0fe890de85","9665e896e21e44f8b1a4485a44cfce5f","cca6381143504519baee72177bd719df","12275f8dde894b93af6d5d755ec54b88","8e5ce6fe84094c5abb33aea0116d3005","6ff830f32b114f80b9f7c0aad189977b","e6148113b1da48ddb805018493c26116","35b2eb93f02944649aaa5a0e4e908826","6811a1429a2a4c3080f480c536b2e799","1adff83c9a2f4ec799a01861ab67e003","b4d5bf1f305549bfae021988847682a3","d0e92e7956ca4dc19386eaba34287cd2","ba31eb57983a4c988f034a28ec3a091c","f4de39b682ad459b9c97f90ca980896c","55e79a110b9e414eabbdd60169bf5f26","ebaaca5ae23c47a7a58029250d719645","209ad52252064ae1b2ba0556859c2778","952d301d90c34f2a87a44f4609a111f1","33896dbb377c48e7b34a4c15ec97dc35","2137b40102154478b14fbb51232bebd2"]},"id":"B055tRCusvyu","outputId":"7df907a6-d990-45a9-c0bf-38a6f5f067da"},"outputs":[],"source":["train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-K1YhFLs_Kt"},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svFvQBOmtYMU"},"outputs":[],"source":["# from google.colab import runtime\n","# runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPCKz1PLAociifVsx3055cE","collapsed_sections":["n8h_LG99l3Nf","h2aLmC5nZa6h"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03a5b874b7874023b6021b0fe890de85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ff830f32b114f80b9f7c0aad189977b","max":352,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6148113b1da48ddb805018493c26116","value":0}},"12275f8dde894b93af6d5d755ec54b88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1adff83c9a2f4ec799a01861ab67e003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4d5bf1f305549bfae021988847682a3","IPY_MODEL_d0e92e7956ca4dc19386eaba34287cd2","IPY_MODEL_ba31eb57983a4c988f034a28ec3a091c"],"layout":"IPY_MODEL_f4de39b682ad459b9c97f90ca980896c"}},"209ad52252064ae1b2ba0556859c2778":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2137b40102154478b14fbb51232bebd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"218563f8f8a849b69b92053370fcce79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7df737d98f140159597180c82ae9b6b","placeholder":"​","style":"IPY_MODEL_e9a2229058cd4caabf47b6644a05eafc","value":" 2/2 [00:03&lt;00:00,  0.66it/s]"}},"22ce04bd9c424885b5170eb807e88000":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac10c65c1f9a4637a80bbb37fa97bd99","IPY_MODEL_c0de1aec2aa342aeb1ad23f71858412f","IPY_MODEL_218563f8f8a849b69b92053370fcce79"],"layout":"IPY_MODEL_32f47b5e376c44edb9657f669322c0f1"}},"26bc5cf831b54ecab4bdfda72b14d785":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32f47b5e376c44edb9657f669322c0f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"33896dbb377c48e7b34a4c15ec97dc35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35b2eb93f02944649aaa5a0e4e908826":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3662572bc60a4d4f9219f3680c0f15b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55e79a110b9e414eabbdd60169bf5f26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6811a1429a2a4c3080f480c536b2e799":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ff830f32b114f80b9f7c0aad189977b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e5ce6fe84094c5abb33aea0116d3005":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"952d301d90c34f2a87a44f4609a111f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9665e896e21e44f8b1a4485a44cfce5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35b2eb93f02944649aaa5a0e4e908826","placeholder":"​","style":"IPY_MODEL_6811a1429a2a4c3080f480c536b2e799","value":" 0/352 [00:00&lt;?, ?it/s, v_num=l72t]"}},"a18eb1f168d443e680ccea951fde3529":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f592a0b632384704a673df9a7ba87c2a","IPY_MODEL_03a5b874b7874023b6021b0fe890de85","IPY_MODEL_9665e896e21e44f8b1a4485a44cfce5f"],"layout":"IPY_MODEL_cca6381143504519baee72177bd719df"}},"a33cbd7efa464c6e8d76c27d3a0d3fec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac10c65c1f9a4637a80bbb37fa97bd99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a33cbd7efa464c6e8d76c27d3a0d3fec","placeholder":"​","style":"IPY_MODEL_3662572bc60a4d4f9219f3680c0f15b1","value":"Sanity Checking DataLoader 0: 100%"}},"b26a59549f5f420598cd5c3dc7bf14c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4d5bf1f305549bfae021988847682a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55e79a110b9e414eabbdd60169bf5f26","placeholder":"​","style":"IPY_MODEL_ebaaca5ae23c47a7a58029250d719645","value":"Validation DataLoader 0: 100%"}},"ba31eb57983a4c988f034a28ec3a091c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33896dbb377c48e7b34a4c15ec97dc35","placeholder":"​","style":"IPY_MODEL_2137b40102154478b14fbb51232bebd2","value":" 40/40 [00:11&lt;00:00,  3.43it/s]"}},"c0de1aec2aa342aeb1ad23f71858412f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_26bc5cf831b54ecab4bdfda72b14d785","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b26a59549f5f420598cd5c3dc7bf14c4","value":2}},"cca6381143504519baee72177bd719df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d0e92e7956ca4dc19386eaba34287cd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_209ad52252064ae1b2ba0556859c2778","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_952d301d90c34f2a87a44f4609a111f1","value":40}},"e6148113b1da48ddb805018493c26116":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9a2229058cd4caabf47b6644a05eafc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebaaca5ae23c47a7a58029250d719645":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4de39b682ad459b9c97f90ca980896c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"f592a0b632384704a673df9a7ba87c2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12275f8dde894b93af6d5d755ec54b88","placeholder":"​","style":"IPY_MODEL_8e5ce6fe84094c5abb33aea0116d3005","value":"Epoch 1:   0%"}},"f7df737d98f140159597180c82ae9b6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
